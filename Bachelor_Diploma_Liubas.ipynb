{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nP3g92Hy-qe"
      },
      "source": [
        "# Goal data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6boK9-2y-qi"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxQcEkxjGebR"
      },
      "outputs": [],
      "source": [
        "# !pip install meteostat\n",
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DiVX0bwy-qj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from math import sqrt\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "from scipy.stats import gennorm\n",
        "import scipy.stats\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import johnsonsu\n",
        "from scipy.stats import exponnorm\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import mstats\n",
        "\n",
        "from prophet import Prophet\n",
        "import itertools\n",
        "from prophet.diagnostics import performance_metrics\n",
        "from prophet.diagnostics import cross_validation\n",
        "from prophet.plot import plot_plotly, plot_components_plotly\n",
        "from prophet.serialize import model_to_json, model_from_json\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from meteostat import Point, Daily, Hourly\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import expand_dims\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Bidirectional, Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import holidays\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWNpljA5Gm1S"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqAWToQmy-qk"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33R5_VwofIeZ"
      },
      "outputs": [],
      "source": [
        "def mape(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUXVUKPliQPr"
      },
      "outputs": [],
      "source": [
        "def plot_actual_forecasted(actual, predicted):\n",
        "    '''\n",
        "    predicted, actual: pd.Series. actual should have timestamp index\n",
        "    '''\n",
        "    plt.plot(actual.index, actual.values, label='Actual')\n",
        "    plt.plot(actual.index, predicted.values, label='Forecasted')\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2R4_JCny-ql"
      },
      "outputs": [],
      "source": [
        "def smooth_data(df, basis=\"D\", func='mean'):\n",
        "    '''\n",
        "    averages data according to some rule and returns new dataframe\n",
        "    params:\n",
        "    df: pd.DataFrame, dataframe to smooth data\n",
        "    basis: str, on which basis to average a data, can be \"D\", \"W\", \"M\", \"Y\"\n",
        "    '''\n",
        "    if func == 'mean':\n",
        "        return df.resample(rule=basis).mean()\n",
        "    if func == 'sum':\n",
        "        return df.resample(rule=basis).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJU_yDvXy-ql"
      },
      "outputs": [],
      "source": [
        "def set_datetime_index(df, date_name):\n",
        "    '''\n",
        "    df: pd.DataFrame\n",
        "    date_name: str, name of column that should be date time index\n",
        "    returns dataFrame with new index\n",
        "    '''\n",
        "    df = df.set_index(date_name)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DyRNZpPrqGm"
      },
      "outputs": [],
      "source": [
        "def mutual_info_with_df(other_df, col_other, price_df=None):\n",
        "    try:\n",
        "        price_df=gen_rt_df\n",
        "    except NameError:\n",
        "        return \"Please read generator data first\"\n",
        "    for i in ['M', 'W', 'D', 'H', '30min', '5min']:\n",
        "        print('\\n\\n ', i, ':')\n",
        "        temp_other = smooth_data(other_df, i)[col_other].dropna()\n",
        "        temp_gen = smooth_data(price_df, i).dropna()\n",
        "        temp_df = temp_gen.merge(temp_other, how='inner', left_index=True, right_index=True)\n",
        "        # print('LBMP: ', mutual_info_score(temp_other['Load'], temp_gen['LBMP ($/MWHr)']))\n",
        "        print('Marginal price: ', mutual_info_score(temp_df[col_other], temp_df['Marginal energy']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5eTWlmjOG_o"
      },
      "outputs": [],
      "source": [
        "def get_param_grid(params_grid):\n",
        "    '''\n",
        "    params_grid: dictionary of parameters of the next structure:\n",
        "    params_grid = {\n",
        "    'growth': ['linear', 'logistic'],\n",
        "    'changepoint_prior_scale': [0.01, 0.1, 1.0],\n",
        "    }\n",
        "    Returns list of combinations of these parameters, like\n",
        "    [{'growth': 'logistic',\n",
        "    'changepoint_prior_scale': 0.01}, {...}]\n",
        "    '''\n",
        "    param_values = list(params_grid.values())\n",
        "    param_names = list(params_grid.keys())\n",
        "\n",
        "    param_combinations = itertools.product(*param_values)\n",
        "\n",
        "    param_grid_list = []\n",
        "    for combination in param_combinations:\n",
        "        param_dict = {}\n",
        "        for i in range(len(param_names)):\n",
        "            param_dict[param_names[i]] = combination[i]\n",
        "        param_grid_list.append(param_dict)\n",
        "\n",
        "    return param_grid_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O68bAiGs-CM8"
      },
      "outputs": [],
      "source": [
        "def plot_corr_plot(df_corr, figsize=(10,8), cmap=\"coolwarm\"):\n",
        "    '''\n",
        "    df_corr: pd.DataFrame created by function pd.DF.corr()\n",
        "    '''\n",
        "    mask = np.zeros_like(df_corr, dtype=bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    df_corr[mask] = np.nan\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    g = sns.heatmap(df_corr, cmap=\"coolwarm\", annot=True)\n",
        "    g.set_xticklabels(g.get_xticklabels(),\n",
        "                      rotation = 60,\n",
        "                      # fontsize = 8\n",
        "                      )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRbNqcY_GPIj"
      },
      "outputs": [],
      "source": [
        "def plot_autocorr(data, lags=100, figsize=(15, 10)):\n",
        "    '''\n",
        "    Plots autocorrelation function\n",
        "    data: pd.Series\n",
        "    '''\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    plot_acf(x=data, lags=lags, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggI5sOiZy-ql"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyXz07Iuy-qn"
      },
      "outputs": [],
      "source": [
        "lbmp = 'LBMP ($/MWHr)'\n",
        "start_date='2022-01-01'\n",
        "end_date = '2023-03-16'\n",
        "smoothing_param = '3H'\n",
        "chosen_generator = 'ADK S GLENS___FALLS'\n",
        "\n",
        "train_percentile = 0.8\n",
        "valid_percentile = 0.975\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaAnE-Nhy-qn"
      },
      "source": [
        "## Generator real time LBPM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjpMrlLr0Zx7"
      },
      "source": [
        "### Data reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLH4DZa25mEb"
      },
      "outputs": [],
      "source": [
        "path_ = '/content/gdrive/MyDrive/Diploma/Data/real_time_generator_one_csv/real_time_generator_pivoted.csv'\n",
        "gen_rt_df = set_datetime_index(pd.read_csv(path_), 'Time Stamp')\n",
        "\n",
        "# removing generators where more than 50% of nans\n",
        "na_percentages = gen_rt_df.isna().sum() / len(gen_rt_df)\n",
        "cols_to_remove = na_percentages[na_percentages > 0.5].index.tolist()\n",
        "gen_rt_df.drop(cols_to_remove, axis=1, inplace=True)\n",
        "\n",
        "gen_rt_df = gen_rt_df.loc[start_date:end_date]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO7BbI0v8OTL"
      },
      "source": [
        "### Correlations of LBMPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B-k87_nK7i_"
      },
      "outputs": [],
      "source": [
        "corr_df = gen_rt_df.iloc.corr()\n",
        "\n",
        "# plt.figure(figsize=(110,80))\n",
        "# g = sns.heatmap(corr_df, cmap=\"coolwarm\", annot=True)\n",
        "# g.set_xticklabels(g.get_xticklabels(),\n",
        "#                   rotation = 60,\n",
        "#                   # fontsize = 8\n",
        "#                   )\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3rigNCC61p0"
      },
      "outputs": [],
      "source": [
        "corr_df.describe().loc['mean'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyUTJDlE8aB5"
      },
      "source": [
        "So, in average, correlation of lbmps is 0.74, which is high. And so, choosing features for one is benefecial for all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rLdQ-7m9REw"
      },
      "outputs": [],
      "source": [
        "corr_df['ADK S GLENS___FALLS'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iho7mtWW9e8f"
      },
      "source": [
        "mean correlation of my generator is 0.69, which is quite close to average by dataset, so it should be nice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq5L8vUdy-qp"
      },
      "source": [
        "### Overall analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Overall distribution"
      ],
      "metadata": {
        "id": "PFS1aRrspt5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(gen_rt_df.values.flatten(), bins='auto', edgecolor='blue')\n",
        "\n",
        "plt.xlabel('LBMP')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(-50, 500)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MgHHfUdVpzUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outliers removal"
      ],
      "metadata": {
        "id": "02QpcvlMtK1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing outliers with IQR\n",
        "for gen in gen_rt_df.columns:\n",
        "    sr = pd.DataFrame(gen_rt_df[gen])\n",
        "    quantile25 = sr.quantile(.25)\n",
        "    quantile75 = sr.quantile(.75)\n",
        "    iqr = quantile75 - quantile25\n",
        "    upper = float(quantile75 + 1.5 * iqr)\n",
        "    lower = float(quantile25 - 1.5 * iqr)\n",
        "    sr.loc[(sr[gen] < lower) | (sr[gen] > upper), gen] = np.nan\n",
        "    sr[gen] = sr[gen].fillna(method='ffill')\n",
        "    sr[gen] = sr[gen].fillna(method='bfill')\n",
        "    gen_rt_df[gen] = sr[gen]\n",
        "\n",
        "gen_rt_df.head()"
      ],
      "metadata": {
        "id": "yS8Pk307tNNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868_HJfZy-q9"
      },
      "source": [
        "#### Visualising random timestamp distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBqFyWq9y-q9"
      },
      "outputs": [],
      "source": [
        "grp = gen_rt_df.groupby(pd.Grouper(freq='3H')).apply(lambda x: x.groupby(by='Name').mean())\n",
        "grp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5n3DRtBy-q9"
      },
      "outputs": [],
      "source": [
        "grp = grp.groupby(pd.Grouper(freq='3H', level='Time Stamp'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWMJoZIny-q-"
      },
      "outputs": [],
      "source": [
        "t = list(grp.groups)\n",
        "t[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9btyc5Yzy-q-"
      },
      "outputs": [],
      "source": [
        "n = 0\n",
        "\n",
        "row_num = 3\n",
        "col_num = 6\n",
        "    \n",
        "def visualise_distributions():\n",
        "    fig, axs = plt.subplots(row_num, col_num, figsize=(17, 10))\n",
        "    fig.suptitle(\"Data distributions\")\n",
        "    \n",
        "    for row in range(row_num):\n",
        "        for col in range(col_num):\n",
        "            # if row == row_num-1 and col == col_num-1:\n",
        "            #     break\n",
        "            time_chosen = random.choice(t)\n",
        "            data_grouped = grp.get_group(time_chosen)\n",
        "            data_grouped = data_grouped.groupby('Name').mean()\n",
        "    \n",
        "            data_grouped = data_grouped[data_grouped[lbmp] < data_grouped[lbmp].quantile(0.99)]\n",
        "            data_grouped = data_grouped[data_grouped[lbmp] > data_grouped[lbmp].quantile(0.01)]\n",
        "            \n",
        "            axs[row, col].hist(data_grouped[lbmp], bins='auto' )#density=True)\n",
        "            axs[row, col].set_title(time_chosen)\n",
        "            # index += 1\n",
        "    \n",
        "visualise_distributions()    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojTPY8Zty-rF"
      },
      "source": [
        "# All other data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46D1kf95y-rH"
      },
      "source": [
        "## Real Time Actual Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6E0m7sbzRoD"
      },
      "outputs": [],
      "source": [
        "pathh = '/content/gdrive/MyDrive/Diploma/Data/real_time_actual_load_one_csv/real_time_actual_load.csv'\n",
        "load_rt_df_sum = pd.read_csv(pathh)\n",
        "load_rt_df_sum = load_rt_df_sum.set_index('Time Stamp')\n",
        "load_rt_df_sum.index = pd.to_datetime(load_rt_df_sum.index)\n",
        "load_rt_df_sum.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nspXj6s5y-rJ"
      },
      "source": [
        "### ACF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_rt_df_sum.head()"
      ],
      "metadata": {
        "id": "eZGk3jJL6uVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(load_rt_df_sum))\n",
        "print(type(temp_df))\n",
        "load_rt_df_sum.values"
      ],
      "metadata": {
        "id": "V0bzPPz79kqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = smooth_data(load_rt_df_sum, smoothing_param)\n",
        "# plot_autocorr(temp_df['Load'], lags=50)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "plot_acf(x=temp_df, lags=list(range(50)), ax=ax)"
      ],
      "metadata": {
        "id": "tssuknB05_w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNHqkpHqy-rJ"
      },
      "outputs": [],
      "source": [
        "load_rt_df_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgaCoiVBy-rJ"
      },
      "outputs": [],
      "source": [
        "tmp = smooth_data(load_rt_df_sum, 'H').dropna()\n",
        "\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "plot_acf(x=tmp['Load'], lags=list(range(100)), ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8IWIPjgy-rL"
      },
      "source": [
        "## Balancing Market Advisory\n",
        "Bids, schedules, etc. seems like actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIJvZofICbzu"
      },
      "outputs": [],
      "source": [
        "def read_balancing_market(time_stamps='All'):\n",
        "    '''\n",
        "    time_stamps: int, number of files to read. Can be \"All\"\n",
        "    '''\n",
        "    path = '/content/gdrive/MyDrive/Diploma/Data/BalancingMarketAdvisory/'\n",
        "\n",
        "    filenames = []\n",
        "\n",
        "    for entry in os.listdir(path):\n",
        "        if os.path.isfile(os.path.join(path, entry)):\n",
        "            filenames.append(entry)\n",
        "    filenames.sort(reverse=True)\n",
        "\n",
        "    # market_rt_df = pd.DataFrame(columns=['Start Time', 'Name', 'PTID', 'LBMP ($/MWHr)',\n",
        "    #                                   'Marginal Cost Losses ($/MWHr)', 'Marginal Cost Congestion ($/MWHr)']).set_index('Start Time')\n",
        "    i = 0\n",
        "    # TODO remove slice\n",
        "    if time_stamps=='All':\n",
        "        time_stamps = len(filenames)\n",
        "    for filename in filenames[:time_stamps]:\n",
        "        print(f'{i}/{len(filenames)}')\n",
        "        i += 1\n",
        "        with zipfile.ZipFile(path + filename) as zipobj:\n",
        "            inside_csvs = zipobj.namelist()\n",
        "            for inside_csv in inside_csvs:\n",
        "                temp_market_rt_df = pd.read_csv(zipobj.open(inside_csv))\n",
        "\n",
        "                ind = temp_market_rt_df[temp_market_rt_df['Start Time'] == 'Total'].index\n",
        "                temp_market_rt_df = temp_market_rt_df.drop(ind)\n",
        "\n",
        "                temp_market_rt_df = temp_market_rt_df.set_index('Start Time')\n",
        "\n",
        "                try:\n",
        "                    market_rt_df = pd.concat([market_rt_df, temp_market_rt_df])\n",
        "                except NameError:\n",
        "                    market_rt_df = temp_market_rt_df\n",
        "\n",
        "    market_rt_df.index = pd.to_datetime(market_rt_df.index)\n",
        "\n",
        "    # TODO remove duplicates?\n",
        "    # gen_rt_df['Marginal energy'] = gen_rt_df['LBMP ($/MWHr)'] - gen_rt_df['Marginal Cost Losses ($/MWHr)'] + gen_rt_df['Marginal Cost Congestion ($/MWHr)']\n",
        "    market_rt_df = market_rt_df.sort_values('Start Time')\n",
        "\n",
        "    market_rt_df = market_rt_df[~market_rt_df.index.duplicated(keep='first')]\n",
        "\n",
        "    market_rt_df = market_rt_df.loc[start_date:end_date]\n",
        "\n",
        "    return market_rt_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke8yD1pKFo4B"
      },
      "outputs": [],
      "source": [
        "market_rt_dff = read_balancing_market()\n",
        "market_rt_dff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_t3QDu0L41Y"
      },
      "source": [
        "things to include: Generation Scheduled (0.25), Wheel Throughs Bid (0.25), Gross Imports NYISO (0.25). Maybe Gross Exports HQ (0.21), Gross Exports PJM (0.20)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8KlNfnHexU"
      },
      "outputs": [],
      "source": [
        "# market_rt_dff[lbmp] = smooth_data(gen_rt_df[lbmp], 'H')\n",
        "# market_rt_dff.corr()[lbmp]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3AXrCjdII31"
      },
      "source": [
        "#### ACF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aboPK7s79nQq"
      },
      "outputs": [],
      "source": [
        "plot_autocorr(market_rt_dff['Wheel Throughs Bid'], lags=100, figsize=(8, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDA7cfqEy-rM"
      },
      "outputs": [],
      "source": [
        "# TODO pay attention: just take generation capacity bid and generation scheduled as only needed info. I also will have a price, so it can be estimated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDle6lH0y-rM"
      },
      "outputs": [],
      "source": [
        "# TODO calculate how this info is related to "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "napUzKSDIP1u"
      },
      "outputs": [],
      "source": [
        "plot_autocorr(market_rt_dff['Gross Exports HQ'], lags=100, figsize=(8, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eSgrd7rJIIW"
      },
      "outputs": [],
      "source": [
        "plot_autocorr(market_rt_dff['Gross Exports PJM'], lags=100, figsize=(8, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivaGhJ-hJi1Q"
      },
      "outputs": [],
      "source": [
        "plot_autocorr(market_rt_dff['Gross Imports NYISO'], lags=100, figsize=(8, 5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKk4mnJ9JsIW"
      },
      "outputs": [],
      "source": [
        "plot_autocorr(market_rt_dff['Generation Scheduled'], lags=100, figsize=(8, 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX5AuCLNy-rQ"
      },
      "source": [
        "## Natural gas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvztZzmny-rQ"
      },
      "outputs": [],
      "source": [
        "path_gas = \"/content/gdrive/MyDrive/Diploma/Data/natural_gas/Gas_Historical_Data.csv\"\n",
        "gas_df = pd.read_csv(path_gas)\n",
        "gas_df = gas_df.set_index(\"Date\")\n",
        "gas_df.index = pd.to_datetime(gas_df.index)\n",
        "gas_df = gas_df.sort_index(ascending=True)\n",
        "gas_df = gas_df.loc[start_date:end_date]\n",
        "gas_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xxc9BqKy-rQ"
      },
      "outputs": [],
      "source": [
        "plt.plot(gas_df.index, gas_df['Close/Last'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWKyHN1jAo4Z"
      },
      "source": [
        "### ACF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_NyNLD8AqK6"
      },
      "outputs": [],
      "source": [
        "plot_autocorr(gas_df['Close/Last'], lags=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHx9BPnPy-rS"
      },
      "source": [
        "## Coal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuFJVDWky-rS"
      },
      "outputs": [],
      "source": [
        "coal_df = pd.read_csv('/content/gdrive/MyDrive/Diploma/Data/Coal/Coal_03_17_23-10_16_06.csv')\n",
        "coal_df = coal_df.set_index(\"Date\")\n",
        "coal_df.index = pd.to_datetime(coal_df.index)\n",
        "coal_df = coal_df.loc[start_date:end_date]\n",
        "\n",
        "coal_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP20NiZoy-rS"
      },
      "outputs": [],
      "source": [
        "plt.plot(coal_df.index, coal_df['Close'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrcU81sy-rS"
      },
      "source": [
        "## Weather data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVjExa8A2nYY"
      },
      "outputs": [],
      "source": [
        "# works only from second try, for some reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ0N42Cky-rS"
      },
      "outputs": [],
      "source": [
        "lst_start = start_date.split('-')\n",
        "lst_end = end_date.split('-')\n",
        "\n",
        "start = datetime(int(lst_start[0]), int(lst_start[1]), int(lst_start[2]), hour=00)\n",
        "end = datetime(int(lst_end[0]), int(lst_end[1]), int(lst_end[2])+1)\n",
        "\n",
        "# Create Point for Vancouver, BC\n",
        "new_york = Point(40.805877, -74.409130, 70)\n",
        "\n",
        "# can be hourly\n",
        "data = Hourly(new_york, start, end)\n",
        "data = data.fetch()\n",
        "\n",
        "# Plot line chart including average, minimum and maximum temperature\n",
        "# data.plot(y=['prcp'], figsize=(30, 20))   #, 'tmin', 'tmax'])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knINiVvWyBdt"
      },
      "outputs": [],
      "source": [
        "data[['temp', 'pres', 'dwpt', 'rhum', 'prcp', 'wspd', 'pres']].corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppgrmdb-y-rT"
      },
      "source": [
        "## Crude Oil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qehIjfYBy-rT"
      },
      "outputs": [],
      "source": [
        "oil_df = pd.read_csv('/content/gdrive/MyDrive/Diploma/Data/Crude_Oil/Oil (WTI)_03_17_23-01_20_05.csv')\n",
        "oil_df = oil_df.set_index(\"Date\")\n",
        "oil_df.index = pd.to_datetime(oil_df.index)\n",
        "oil_df = oil_df.loc[start_date:end_date]\n",
        "oil_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am9jhSiBy-rU"
      },
      "outputs": [],
      "source": [
        "plt.plot(oil_df.index, oil_df['Close'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd9J43tPy-rU"
      },
      "outputs": [],
      "source": [
        "for i in corr_types:\n",
        "    print(i, \": \", oil_df['Close'].corr(smooth_data(df_main, \"D\")['DAM Gen LBMP'], method=i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg0yivy2y-rU"
      },
      "source": [
        "## Uranium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCiwrytyy-rU"
      },
      "outputs": [],
      "source": [
        "u_df = pd.read_csv('/content/gdrive/MyDrive/Diploma/Data/Uranium/URA Historical Data 2010-2023.csv')\n",
        "u_df = u_df.set_index(\"Date\")\n",
        "u_df.index = pd.to_datetime(u_df.index)\n",
        "u_df = u_df.loc[start_date:end_date]\n",
        "u_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxBKwjQ0y-rU"
      },
      "outputs": [],
      "source": [
        "plt.plot(u_df.index, u_df['Price'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-Il78oM_b2"
      },
      "source": [
        "# Models training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBwUhMstutLd"
      },
      "source": [
        "## Models preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gC1dGGluxjt"
      },
      "outputs": [],
      "source": [
        "class GenericModel:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.default_path = '/content/gdrive/MyDrive/Diploma/Models/'\n",
        "        self.default_extention = '.pkl'\n",
        "        return None\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, path):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def read_params(self, path):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HMQiRza28vs"
      },
      "source": [
        "### Prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXtkMoqBv6wA"
      },
      "outputs": [],
      "source": [
        "class MyFbProphet(GenericModel):\n",
        "    param_grid = {\n",
        "                  'growth': ['linear'], #, 'logistic'],\n",
        "                  'changepoint_prior_scale': [0.01, 0.1, 0.5,    0.001],\n",
        "                  'seasonality_mode': ['additive', 'multiplicative'],\n",
        "                  'seasonality_prior_scale': [0.1, 5.0,     1, 10],\n",
        "                  # TODO del?\n",
        "                  # 'changepoint_range': [0.8, 0.95],\n",
        "                  # 'holidays_prior_scale': [0.01, 1, 10]\n",
        "                  }\n",
        "    name = 'prophet_model'\n",
        "\n",
        "    def __init__(self, features, params=dict()):\n",
        "        '''\n",
        "        features: list of column names of train_x to add to model\n",
        "        params: dictionary of parameters\n",
        "        '''\n",
        "        super(MyFbProphet, self).__init__()\n",
        "        self.model = Prophet(**params, stan_backend=None)\n",
        "        self.features = features\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        '''\n",
        "        \n",
        "        '''\n",
        "        for feature in self.features:\n",
        "            self.model.add_regressor(feature)\n",
        "        train_x['ds'] = train_x.index\n",
        "        train_x['y'] = train_y.values\n",
        "        self.model.fit(train_x[['ds', 'y'] + self.features])\n",
        "        train_x.drop(['ds', 'y'], axis=1, inplace=True)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        test_x['ds'] = test_x.index\n",
        "        result = self.model.predict(test_x[['ds'] + self.features])['yhat']\n",
        "        test_x.drop(['ds'], axis=1, inplace=True)\n",
        "        return result\n",
        "\n",
        "    def save(self, name=None,  parameters=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        joblib.dump(parameters, work_path)\n",
        "        # with open(work_path, 'w') as fout:\n",
        "        #     fout.write(model_to_json(self.model))\n",
        "\n",
        "    def read_params(self, name=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        params = joblib.load(work_path)\n",
        "        self.model = Prophet(**params)\n",
        "        # with open(work_path, 'r') as fin:\n",
        "        #     self.model = model_from_json(fin.read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKYq9fLovRno"
      },
      "source": [
        "### Gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44UbGeq7vQtk"
      },
      "outputs": [],
      "source": [
        "class MyGradientBoosting(GenericModel):\n",
        "    param_grid = {\n",
        "                  'n_estimators': [100, 300],\n",
        "                  'max_depth': [4, 7],\n",
        "                  # 0.01 does not show good results.\n",
        "                  'learning_rate': [0.1], #, 0.01],\n",
        "                  'subsample': [0.8, 0.9],\n",
        "                  'colsample_bytree': [0.8, 0.9]\n",
        "                  }\n",
        "    name = 'gradient_boosting_model'\n",
        "\n",
        "    def __init__(self, features, params=dict()):\n",
        "        super(MyGradientBoosting, self).__init__()\n",
        "        self.model = XGBRegressor(**params)\n",
        "        self.features = features\n",
        "        \n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        self.model.fit(train_x[self.features], train_y)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        result = self.model.predict(test_x[self.features])\n",
        "        return result\n",
        "\n",
        "    def save(self, name=None, parameters=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        if not parameters:\n",
        "            parameters = self.model.get_params()\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        joblib.dump(parameters, work_path)\n",
        "\n",
        "    def read_params(self, name=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        params = joblib.load(work_path)\n",
        "        self.model = XGBRegressor(**params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBd-cCvGhNOX"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dETnK7eQg7sT"
      },
      "outputs": [],
      "source": [
        "class MyRandomForest(GenericModel):\n",
        "    param_grid = {\n",
        "                  'n_estimators': [100, 300],\n",
        "                  'max_depth': [4, 8],\n",
        "                  'min_samples_split': [2, 10],\n",
        "                  'min_samples_leaf': [1, 4],\n",
        "                  'max_features': [1, 'sqrt']\n",
        "                  }\n",
        "    name = 'random_forest_model'\n",
        "\n",
        "    def __init__(self, features, params=dict()):\n",
        "        super(MyRandomForest, self).__init__()\n",
        "        self.model = RandomForestRegressor(**params)\n",
        "        self.features = features\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        self.model.fit(train_x[self.features], train_y)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        result = self.model.predict(test_x[self.features])\n",
        "        return result\n",
        "\n",
        "    def save(self, name=None, parameters=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        if not parameters:\n",
        "            parameters = self.model.get_params()\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        joblib.dump(self.model.get_params(), work_path)\n",
        "\n",
        "    def read_params(self, name=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        params = joblib.load(work_path)\n",
        "        self.model = RandomForestRegressor(**params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LI8C1gwi7hq"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZx4PiARi90c"
      },
      "outputs": [],
      "source": [
        "class MyLinearRegression(GenericModel):\n",
        "    param_grid = {\n",
        "                  'fit_intercept': [True, False],\n",
        "                  'n_jobs': [None, 100],\n",
        "                  }\n",
        "    name = 'linear_regression_model'\n",
        "\n",
        "    def __init__(self, features, params=dict()):\n",
        "        super(MyLinearRegression, self).__init__()\n",
        "        self.model = LinearRegression()\n",
        "        self.model.set_params(**params)\n",
        "        self.features = features\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        self.model.fit(train_x[self.features], train_y)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        result = self.model.predict(test_x[self.features])\n",
        "        return result\n",
        "\n",
        "    def save(self, name=None, parameters=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        if not parameters:\n",
        "            parameters = self.model.get_params()\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        joblib.dump(self.model.get_params(), work_path)\n",
        "\n",
        "    def read_params(self, name=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        params = joblib.load(work_path)\n",
        "        self.model = LinearRegression(**params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDgCqCjQmY9t"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLZzACH2nf13"
      },
      "outputs": [],
      "source": [
        "class MyCNN(GenericModel):\n",
        "    param_grid = {\n",
        "                  'filters': [32, 64, 128],\n",
        "                  'kernel_size': [3, 5],\n",
        "                  'dense_layer_size': [50, 100]\n",
        "                  }\n",
        "    name = 'cnn_model'\n",
        "\n",
        "    def __init__(self, features, params=dict()):\n",
        "        super(MyCNN, self).__init__()\n",
        "        \n",
        "        if len(params) == 0:\n",
        "            self.model = self.create_model()\n",
        "        else:\n",
        "            self.model = self.create_model(**params)     \n",
        "        self.features = features\n",
        "\n",
        "    def create_model(self, filters=64, kernel_size=3, dense_layer_size=50):\n",
        "        mdl = Sequential()\n",
        "        mdl.add(Conv1D(filters=filters,\n",
        "                         kernel_size=kernel_size,\n",
        "                         activation='relu',\n",
        "                         input_shape=(len(features), 1)))\n",
        "        mdl.add(MaxPooling1D(pool_size=2))\n",
        "        mdl.add(Flatten())\n",
        "        mdl.add(Dense(dense_layer_size, activation='relu'))\n",
        "        mdl.add(Dense(1))\n",
        "        mdl.compile(optimizer='adam',\n",
        "                      loss='mean_absolute_percentage_error')\n",
        "        return mdl\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        train_x = train_x[self.features]\n",
        "        train_x_cnn = np.reshape(train_x.values,\n",
        "                                 (train_x.shape[0], train_x.shape[1], 1))\n",
        "        # verbose: 0 - see nothing, 1 - see animation, 2 - see epoch count\n",
        "        self.model.fit(train_x_cnn, train_y,\n",
        "                       epochs=15, batch_size=16, verbose=0)\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        test_x = test_x[self.features]\n",
        "        test_x_cnn = np.reshape(test_x.values,\n",
        "                                (test_x.shape[0], test_x.shape[1], 1))\n",
        "        y_pred = self.model.predict(test_x_cnn)\n",
        "        result = y_pred.reshape(-1)\n",
        "        return result\n",
        "\n",
        "    def save(self, name=None, parameters=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        joblib.dump(parameters, work_path)\n",
        "        # self.model.save(work_path)\n",
        "\n",
        "    def read_params(self, name=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        params = joblib.load(work_path)\n",
        "        self.model = self.create_model(**params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mv2B-zwxKps"
      },
      "source": [
        "### LSTM (DEPRECATED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31zZYlpxNvC"
      },
      "outputs": [],
      "source": [
        "class MyLSTM(GenericModel):\n",
        "    param_grid = {\n",
        "                  'optimizer': ['adam'],\n",
        "                  'neurons': [10, 50, 100],\n",
        "                  'activation': ['relu', 'sigmoid'],\n",
        "                  'dropout': [0.2, 0.4]\n",
        "                  }\n",
        "    name = 'lstm_model'\n",
        "\n",
        "    def __init__(self, features, params=dict()):\n",
        "        super(MyLSTM, self).__init__()\n",
        "        \n",
        "        if len(params) == 0:\n",
        "            self.model = self.create_model()\n",
        "        else:\n",
        "            self.model = self.create_model(**params)     \n",
        "        self.features = features\n",
        "\n",
        "    def create_model(self, optimizer='adam', neurons=50, activation='relu', dropout=0.2):\n",
        "        mdl = Sequential([ \n",
        "        Bidirectional(LSTM(units=neurons, input_shape=(1, len(features)),\n",
        "                           activation=activation,\n",
        "                           return_sequences=True)),\n",
        "        Dropout(dropout),\n",
        "        # Bidirectional(LSTM(units=neurons, return_sequences=True)),\n",
        "        # Dropout(dropout),\n",
        "        Bidirectional(LSTM(units=neurons, return_sequences=False)),\n",
        "        Dropout(dropout),\n",
        "        Dense(1),\n",
        "        ])\n",
        "\n",
        "        mdl.compile(optimizer=optimizer, loss='mean_absolute_percentage_error')\n",
        "        return mdl\n",
        "\n",
        "    def fit(self, train_x, train_y):\n",
        "        self.scaler = StandardScaler()\n",
        "        train_x_scaled = self.scaler.fit_transform(train_x)\n",
        "        \n",
        "        n_features = len(features) # train_x_scaled.shape[1]\n",
        "        train_x_lstm = train_x_scaled.reshape(train_x_scaled.shape[0], 1, n_features)\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, mode='min')\n",
        "\n",
        "        self.model.fit(train_x_lstm, train_y,\n",
        "                       epochs=15,\n",
        "                       batch_size=32,\n",
        "                       verbose=0,\n",
        "                       callbacks=[early_stop])\n",
        "\n",
        "    def predict(self, test_x):\n",
        "        test_x_scaled = self.scaler.transform(test_x)\n",
        "        \n",
        "        test_x_lstm = test_x_scaled.reshape(test_x_scaled.shape[0],\n",
        "                                            1, len(features))\n",
        "        result = self.model.predict(test_x_lstm)\n",
        "        return result\n",
        "\n",
        "    def save(self, name=None, parameters=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        joblib.dump(parameters, work_path)\n",
        "        # self.model.save(work_path)\n",
        "\n",
        "    def read_params(self, name=None):\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        work_path = self.default_path + name + self.default_extention\n",
        "        params = joblib.load(work_path)\n",
        "        self.model = self.create_model(**params)\n",
        "        # self.model = load_model(work_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG9QpjtZ2duH"
      },
      "source": [
        "## Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIi0A6ib2gqf"
      },
      "outputs": [],
      "source": [
        "# data_df = smooth_data(gen_rt_df, 'H')\n",
        "\n",
        "data_df = pd.DataFrame(gen_rt_df[chosen_generator].rename(lbmp, inplace=True))\n",
        "\n",
        "data_df['Load'] = smooth_data(load_rt_df_sum, 'H')\n",
        "data_df = data_df[[lbmp, 'Load']]\n",
        "\n",
        "data_df['Natural Gas'] = gas_df['Close/Last']\n",
        "data_df['Coal'] = coal_df['Close']\n",
        "data_df['Crude Oil'] = oil_df['Close']\n",
        "data_df['Uranium'] = u_df['Price']\n",
        "\n",
        "data_df['Temperature'] = data['temp']\n",
        "data_df['Precip'] = data['prcp']\n",
        "data_df['Precip']= data_df['Precip'].fillna(0)\n",
        "data_df['Pressure'] = data['pres']\n",
        "data_df['Wind speed'] = data['wspd']\n",
        "data_df['Dew temperature'] = data['dwpt']\n",
        "data_df['Relative humidity'] = data['rhum']\n",
        "\n",
        "data_df['Gen Scheduled'] = market_rt_dff['Generation Scheduled']\n",
        "data_df['Wheel Throughs Bid'] = market_rt_dff['Wheel Throughs Bid']\n",
        "data_df['Imports NYISO'] = market_rt_dff['Gross Imports NYISO']\n",
        "data_df['Exports HQ'] = market_rt_dff['Gross Exports HQ']\n",
        "data_df['Exports PJM'] = market_rt_dff['Gross Exports PJM']\n",
        "\n",
        "data_df = data_df.fillna(method='ffill')\n",
        "data_df = data_df.fillna(method='bfill')\n",
        "\n",
        "us_holidays = holidays.US()\n",
        "data_df['Is holiday'] = pd.Series(data_df.index).apply(lambda x: 1 if x in us_holidays else 0).values\n",
        "\n",
        "data_df = data_df.loc[start_date:end_date]\n",
        "data_df = smooth_data(data_df, smoothing_param)\n",
        "\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKWsx5qEPHZd"
      },
      "outputs": [],
      "source": [
        "# features = ['Load', 'Natural Gas', 'Coal', 'Crude Oil', 'Uranium',\n",
        "#             'Temperature', 'Precip', 'Pressure', 'Wind speed',\n",
        "#             'Dew temperature', 'Relative humidity', 'Is holiday', \n",
        "#             'Gen Scheduled', 'Wheel Throughs Bid', 'Imports NYISO',\n",
        "#             'Exports HQ', 'Exports PJM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC9HDKOVSSsV"
      },
      "outputs": [],
      "source": [
        "# TODO move up?\n",
        "generators_df = smooth_data(gen_rt_df.loc[start_date:end_date], smoothing_param)\n",
        "generators_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct6FEdfKvKJZ"
      },
      "outputs": [],
      "source": [
        "data_df[lbmp].plot(figsize=(10, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQGPKgDWSJSV"
      },
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLvzeqpYSLsI"
      },
      "source": [
        "### Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHJxbOOm-rB6"
      },
      "outputs": [],
      "source": [
        "plot_corr_plot(data_df.corr().round(2), figsize=(12, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFFGBZAMSWnP"
      },
      "source": [
        "### Mutual info score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2eIL582SY9q"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "mi_scores = pd.DataFrame(columns=data_df.columns, index=data_df.columns)\n",
        "\n",
        "# Calculate the mutual information scores\n",
        "for i in range(len(data_df.columns)):\n",
        "    for j in range(i, len(data_df.columns)):\n",
        "        if i==j:\n",
        "            continue\n",
        "        mi_score = mutual_info_regression(data_df.iloc[:, i].values.reshape(-1, 1), data_df.iloc[:, j])\n",
        "        mi_score = round(mi_score[0], 2)\n",
        "        # mi_scores.iloc[i, j] = mi_score\n",
        "        mi_scores.iloc[j, i] = mi_score\n",
        "mi_scores = mi_scores.astype(float)\n",
        "\n",
        "mi_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bb5MiX58gQDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mubNq0HgidE0"
      },
      "source": [
        "## Features forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvBzLk6tizns"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yy9I9Sll99M"
      },
      "outputs": [],
      "source": [
        "def fit_predict_prophet(params, train, test, y_name):\n",
        "    '''\n",
        "    train, test: pd.Series\n",
        "    y_name: name of value to be forecasted, e.g. 'Close/Last'\n",
        "    returns predicted value\n",
        "    '''\n",
        "    my_model = MyFbProphet([], params)\n",
        "    my_model.fit(pd.DataFrame(train).drop(y_name, axis=1),\n",
        "                 train)\n",
        "    predicted = my_model.predict(pd.DataFrame(test).drop(y_name, axis=1))\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqxwoenXi1aC"
      },
      "outputs": [],
      "source": [
        "def do_train_valid_test_prophet(train, valid, train_valid, test, y_name):\n",
        "    '''\n",
        "    train, valid, train_valid, test: pd.Series with datetime index\n",
        "    y_name: name of value to be forecasted, e.g. 'Close/Last'\n",
        "\n",
        "    Returns tuple (mape_on_test and best_params)\n",
        "    '''\n",
        "    # validation step\n",
        "    best_mape = float('inf')\n",
        "    for param in get_param_grid(MyFbProphet([]).param_grid):\n",
        "        predicted = fit_predict_prophet(param, train, valid, y_name)\n",
        "        curr_mape = mape(valid.values, predicted)\n",
        "\n",
        "        # print(my_model.name, \": \", curr_mape)\n",
        "        # print('Params: ', param)\n",
        "\n",
        "        if curr_mape <= best_mape:\n",
        "            best_model = MyFbProphet([], param)\n",
        "            best_mape = curr_mape\n",
        "            best_params = param\n",
        "\n",
        "    predicted = fit_predict_prophet(best_params, train_valid, test, y_name)\n",
        "\n",
        "    curr_mape_test = mape(test.values, predicted)\n",
        "\n",
        "    plot_actual_forecasted(test, predicted)\n",
        "\n",
        "    return curr_mape_test, best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU-JlJdtj92N"
      },
      "outputs": [],
      "source": [
        "def return_four_sets(dataset, y_name):\n",
        "    '''\n",
        "    dataset: pd.Series\n",
        "    returns 4 sets of data: train, valid, train_valid, test\n",
        "    '''\n",
        "    train = dataset.iloc[:int(train_percentile * len(dataset))][y_name]\n",
        "    valid = dataset.iloc[int(train_percentile * len(dataset)):int(valid_percentile * len(dataset))][y_name]\n",
        "\n",
        "    train_valid = dataset.iloc[:int(valid_percentile * len(dataset))][y_name]\n",
        "    test = dataset.iloc[int(valid_percentile * len(dataset)):][y_name]\n",
        "\n",
        "    return train, valid, train_valid, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7riJ1p3iiBJ"
      },
      "source": [
        "### Gas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s101RDWkm03"
      },
      "outputs": [],
      "source": [
        "four_sets = return_four_sets(gas_df, 'Close/Last')\n",
        "gas_actual = four_sets[3]\n",
        "gas_mape, gas_params = do_train_valid_test_prophet(four_sets[0], four_sets[1],\n",
        "                                                   four_sets[2], four_sets[3], 'Close/Last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WREUDQ7lusq"
      },
      "outputs": [],
      "source": [
        "predicted = fit_predict_prophet(gas_params, four_sets[2], four_sets[3], 'Close/Last')\n",
        "print(mape(predicted.values, four_sets[3].values))\n",
        "plot_actual_forecasted(four_sets[3], predicted)\n",
        "\n",
        "# TODO IMPORTANT TO RUN\n",
        "gas_test_forecasted = predicted.set_axis(four_sets[3].index)\n",
        "gas_test_forecasted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKMnrc7Y3y8h"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdHZ4_6W30t0"
      },
      "outputs": [],
      "source": [
        "name_of_load = 'Load'\n",
        "\n",
        "load_series = pd.DataFrame(data_df[name_of_load])\n",
        "\n",
        "four_sets = return_four_sets(load_series, name_of_load)\n",
        "load_actual = four_sets[3]\n",
        "load_mape, load_params = do_train_valid_test_prophet(four_sets[0], four_sets[1],\n",
        "                                                     four_sets[2], four_sets[3], name_of_load)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46UWe-2S4e0u"
      },
      "outputs": [],
      "source": [
        "predicted = fit_predict_prophet(load_params, four_sets[2], four_sets[3], name_of_load)\n",
        "print(mape(predicted.values, four_sets[3].values))\n",
        "plot_actual_forecasted(four_sets[3], predicted)\n",
        "\n",
        "# TODO IMPORTANT TO RUN\n",
        "load_test_forecasted = predicted.set_axis(four_sets[3].index)\n",
        "load_test_forecasted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "ra9MXdhyoaLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_angle = 45\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "ax1.plot(gas_actual, label='Actual Gas Price', color='blue')\n",
        "ax1.plot(gas_test_forecasted, label='Predicted Gas Price', color='orange')\n",
        "\n",
        "ax1.set_title('Actual vs Predicted Gas Price')\n",
        "# ax1.set_xlabel('Time')\n",
        "# ax1.set_ylabel('Gas')\n",
        "ax1.tick_params(axis='x', rotation=rotation_angle)\n",
        "ax1.set_ylim(0)\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(load_actual, label='Actual Load', color='blue')\n",
        "ax2.plot(load_test_forecasted, label='Predicted Load', color='orange')\n",
        "\n",
        "ax2.set_title('Actual vs Predicted Load')\n",
        "# ax2.set_xlabel('Time')\n",
        "# ax2.set_ylabel('Load')\n",
        "ax2.tick_params(axis='x', rotation=rotation_angle)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "owbKgS85obSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig.savefig('gas_load_temp.eps', format='eps')"
      ],
      "metadata": {
        "id": "gGLbIKMWteCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vYTLt2TT23m"
      },
      "source": [
        "## Overall training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRi_MncndMOs"
      },
      "outputs": [],
      "source": [
        "features = ['Load', 'Natural Gas', 'Dew temperature', 'Relative humidity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RvKKOmmjT5KX"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "          # MyGradientBoosting,\n",
        "          MyFbProphet,\n",
        "          # MyRandomForest,\n",
        "          # MyLinearRegression,\n",
        "          # MyCNN,\n",
        "          # MyLSTM\n",
        "          ]\n",
        "\n",
        "train_valid_len = int(len(data_df) * valid_percentile)\n",
        "n_folds = 3\n",
        "fold_size = int(train_valid_len/n_folds)\n",
        "best_mape = float('inf')\n",
        "\n",
        "gen_n = 0\n",
        "\n",
        "for generator in generators_df.columns[375:460]:\n",
        "    for m in models:\n",
        "        best_mape = float('inf')\n",
        "        for param in get_param_grid(m(features).param_grid):\n",
        "            mape_scores = []\n",
        "            for i in range(fold_size, train_valid_len+1, fold_size):\n",
        "                train_data = data_df.iloc[:i]\n",
        "                valid_data = data_df.iloc[i:i+fold_size]\n",
        "\n",
        "                my_model = m(features, param)\n",
        "                # my_model.fit(train_data.drop(lbmp, axis=1), train_data[lbmp])\n",
        "                my_model.fit(train_data.drop(lbmp, axis=1),\n",
        "                             generators_df[generator].loc[train_data.index])\n",
        "                predicted = my_model.predict(valid_data.drop(lbmp, axis=1))\n",
        "                # mape_scores.append(mape(valid_data[lbmp].values, predicted))\n",
        "                mape_scores.append(mape(generators_df[generator].loc[valid_data.index].values,\n",
        "                                        predicted))\n",
        "\n",
        "            curr_mape = np.mean(mape_scores)\n",
        "            # print(my_model.name, \": \", curr_mape)\n",
        "            # print('Params: ', param)\n",
        "            if curr_mape <= best_mape:\n",
        "                best_model = my_model\n",
        "                best_mape = curr_mape\n",
        "                best_params = param\n",
        "\n",
        "        best_model.save(generator + '|' + best_model.name, parameters=best_params)\n",
        "    \n",
        "    print('\\n\\n\\n', gen_n, '\\n\\n\\n')\n",
        "    gen_n +=1    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrm7rx4WNE0A"
      },
      "outputs": [],
      "source": [
        "print(best_model)\n",
        "print(best_mape)\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HAfkp68004n"
      },
      "source": [
        "## Reading of saved models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtPmUnAv03Y3"
      },
      "outputs": [],
      "source": [
        "models_dict = {\n",
        "               MyGradientBoosting(features).name: MyGradientBoosting,\n",
        "               MyFbProphet(features).name: MyFbProphet,\n",
        "               MyRandomForest(features).name: MyRandomForest,\n",
        "               MyLinearRegression(features).name: MyLinearRegression,\n",
        "               MyCNN(features).name: MyCNN,\n",
        "              #  MyLSTM(features).name: MyLSTM\n",
        "               }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJdWHJgD2VEW"
      },
      "outputs": [],
      "source": [
        "train_data = data_df.iloc[:int(len(data_df) * train_percentile)]\n",
        "valid_data = data_df.iloc[int(len(data_df) * train_percentile):int(len(data_df) * valid_percentile)]\n",
        "train_val_data = data_df.iloc[:int(len(data_df) * valid_percentile)]\n",
        "test_data = data_df.iloc[int(len(data_df) * valid_percentile):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdqtaQpr3PBl"
      },
      "outputs": [],
      "source": [
        "# updatind of test with forecasted values\n",
        "test_data['Natural Gas'] = gas_test_forecasted\n",
        "test_data['Load'] = load_test_forecasted\n",
        "test_data = test_data.fillna(method='ffill')\n",
        "test_data = test_data.fillna(method='bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWH_vHEv05q9"
      },
      "outputs": [],
      "source": [
        "path = '/content/gdrive/MyDrive/Diploma/Models/'\n",
        "filenames = []\n",
        "for entry in os.listdir(path):\n",
        "        if os.path.isfile(os.path.join(path, entry)):\n",
        "            filenames.append(entry)\n",
        "\n",
        "res_dct = dict()\n",
        "best_mape = float('inf')\n",
        "\n",
        "for filename in filenames:\n",
        "    # if not filename.startswith('|'):\n",
        "    #     continue\n",
        "    if filename.endswith('---cnn_model.pkl'):\n",
        "        gen_name = filename[:-16]\n",
        "        model_name = 'cnn_model'\n",
        "    elif filename.endswith('_cnn_model.pkl'):\n",
        "        gen_name = filename[:-14]\n",
        "        model_name = 'cnn_model'\n",
        "    elif filename.endswith('_prophet_model.pkl'):\n",
        "        gen_name = filename[:-18]\n",
        "        model_name = 'prophet_model'\n",
        "    elif filename.endswith('_random_forest_model.pkl'):\n",
        "        gen_name = filename[:-24]\n",
        "        model_name = 'random_forest_model'\n",
        "    else:\n",
        "        gen_name = filename.split('|')[0]\n",
        "        model_name = filename.split('|')[1].split('.')[0]\n",
        "\n",
        "    # gen_name = filename.split('|')[0]\n",
        "    # model_name = filename.split('|')[1].split('.')[0]\n",
        "\n",
        "\n",
        "    model = models_dict[model_name](features)\n",
        "    # model.read_params(filename.split('.')[0])\n",
        "    model.read_params(filename[:-4])\n",
        "    # model.fit(train_data.drop(lbmp, axis=1), train_data[lbmp])\n",
        "    model.fit(train_data.drop(lbmp, axis=1),\n",
        "              generators_df[gen_name].loc[train_data.index])\n",
        "    predicted = model.predict(valid_data.drop(lbmp, axis=1))\n",
        "    # curr_mape = mape(valid_data[lbmp].values, predicted)\n",
        "    curr_mape = mape(generators_df[gen_name].loc[valid_data.index].values,\n",
        "                     predicted)\n",
        "\n",
        "    print(gen_name)\n",
        "    print(model_name)\n",
        "    print(curr_mape)\n",
        "    print('--------------------------\\n')\n",
        "\n",
        "    try:\n",
        "        res_dct[gen_name]\n",
        "    except KeyError:\n",
        "        res_dct[gen_name] = {\n",
        "                             'best_mape': curr_mape,\n",
        "                             'best_model': model_name\n",
        "                            }\n",
        "        continue\n",
        "    if curr_mape < res_dct[gen_name]['best_mape']:\n",
        "        res_dct[gen_name]['best_mape'] = curr_mape\n",
        "        res_dct[gen_name]['best_model'] = model_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGsSo-jF5V23"
      },
      "outputs": [],
      "source": [
        "# Save dict??\n",
        "\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Diploma/res_dct.json', 'w') as f:\n",
        "    json.dump(res_dct, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/Diploma/res_dct.json', 'r') as f:\n",
        "    res_dct = json.load(f)"
      ],
      "metadata": {
        "id": "hLWuMSrJ-qBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JxLmm1JYWrN"
      },
      "outputs": [],
      "source": [
        "n = 1000\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(valid_data.index[:n], valid_data[lbmp][:n], label='Actual')\n",
        "plt.plot(valid_data.index[:n], predicted[:n], label='Forecasted')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vF2WC_cCCjM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(res_dct, orient='index')\n",
        "\n",
        "# because 200000\n",
        "df = df.drop('SITHE___OGDNSBRG')\n",
        "df.describe()#.boxplot()\n",
        "# 'SITHE___OGDNSBRG'"
      ],
      "metadata": {
        "id": "6DSRHJ9rCfjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # vals = ['mean', 'min', '25%', '50%', '75%', 'max']\n",
        "# df[np.isclose(df['best_mape'],df.describe().loc['75%'], atol=0.01)].iloc[0].name"
      ],
      "metadata": {
        "id": "XTV1KemPFnN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().index"
      ],
      "metadata": {
        "id": "frOnZiBlG7Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib4dPM4uMONh"
      },
      "source": [
        "### Final strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWEC1HQrJSFK"
      },
      "outputs": [],
      "source": [
        "path = '/content/gdrive/MyDrive/Diploma/Models/'\n",
        "\n",
        "final_df_predicted = pd.DataFrame(index=test_data.index)\n",
        "\n",
        "for gen in res_dct.keys():\n",
        "    mdl_name = res_dct[gen]['best_model']\n",
        "    model = models_dict[mdl_name](features)\n",
        "    try:\n",
        "        model.read_params(gen + '|' + mdl_name)\n",
        "    except FileNotFoundError:\n",
        "        try:\n",
        "            model.read_params(gen + '---' + mdl_name)\n",
        "        except FileNotFoundError:\n",
        "            model.read_params(gen + '_' + mdl_name)\n",
        "    # model.fit(train_val_data.drop(lbmp, axis=1), train_val_data[lbmp])\n",
        "    model.fit(train_val_data.drop(lbmp, axis=1),\n",
        "              generators_df[gen].loc[train_val_data.index])\n",
        "    predicted = model.predict(test_data.drop(lbmp, axis=1))\n",
        "    if mdl_name=='prophet_model':\n",
        "        predicted = predicted.set_axis(test_data.index)\n",
        "    final_df_predicted[gen] = predicted\n",
        "final_df_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYGaqzYDVy49"
      },
      "outputs": [],
      "source": [
        "def return_of_strategy(q, predicted_df, actual_df):\n",
        "    list_accepted = []\n",
        "    for i in range(len(predicted_df)):\n",
        "        value = predicted_df.iloc[i].quantile(q)\n",
        "        if value < actual_df.iloc[i].max():\n",
        "            list_accepted.append(value)\n",
        "    return np.sum(list_accepted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruhJkxFe_3Wd"
      },
      "outputs": [],
      "source": [
        "generators_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohL3jQkQXW9I"
      },
      "outputs": [],
      "source": [
        "lst = [i/100 for i in range(1, 101)]\n",
        "res = []\n",
        "\n",
        "for val in lst:\n",
        "    res.append(return_of_strategy(val, final_df_predicted, generators_df.loc[test_data.index]))\n",
        "\n",
        "plt.plot(lst, res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res[90:]"
      ],
      "metadata": {
        "id": "t7a2t69tGwJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "quantile 95 is the best - 2505"
      ],
      "metadata": {
        "id": "VHGk0c7hG-F6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mpwKIpGgcQL"
      },
      "outputs": [],
      "source": [
        "np.max(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22ekhveGkMVp"
      },
      "outputs": [],
      "source": [
        "res_gens = []\n",
        "for gen in generators_df.columns:\n",
        "    res_gens.append(generators_df.loc[test_data.index][gen].sum())\n",
        "\n",
        "print(res_gens)\n",
        "print(np.mean(res_gens))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(res_gens))"
      ],
      "metadata": {
        "id": "0y1BhnuGTk3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(np.array(res_gens), bins='auto', edgecolor='blue', label='Actual earnings \\nof generators')\n",
        "\n",
        "plt.xlabel('$/MWHr')\n",
        "plt.ylabel('Number of generators')\n",
        "# plt.title('Distribution of Values')\n",
        "\n",
        "line_value = 2505\n",
        "plt.axvline(x=line_value, color='r', linestyle='--', label='\\nEarnings with proposed \\nstrategy: {:.0f} $/MWHr'.format(line_value))\n",
        "\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(0, 1), fancybox=True, shadow=True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o9xY8T2EUTpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HW783RiW8JB"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(res_gens).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecasting graph"
      ],
      "metadata": {
        "id": "W1OaPoHFHt9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# axes[0][0]\n",
        "vals = ['min', '25%', '50%', '75%', 'max', 'mean']\n",
        "atol = 3 if q=='mean' else 0.01\n",
        "df[np.isclose(df['best_mape'],df.describe().loc[vals[q]], atol=atol)].iloc[0].name\n"
      ],
      "metadata": {
        "id": "Vuj6FigtRPZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vals = ['min', '25%', '50%', '75%', 'max', 'mean']\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(8, 6), sharex=True)\n",
        "\n",
        "for q in range(len(vals)):\n",
        "    atol = 3 if vals[q]=='mean' else 0.01\n",
        "    name_gen = df[np.isclose(df['best_mape'],df.describe().loc[vals[q]], atol=atol)].iloc[0].name\n",
        "    actual = generators_df[name_gen].loc[final_df_predicted.index]\n",
        "    predicted = final_df_predicted[name_gen]\n",
        "\n",
        "    ax = axes[q//2][q%2]\n",
        "    ax.plot(actual, label='Actual', color='blue')\n",
        "    ax.plot(predicted, label='Predicted', color='orange')\n",
        "    ax.set_title(vals[q])\n",
        "\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    # axes[q//2][q%2].legend()\n",
        "\n",
        "fig.legend(['Actual value', 'Predicted value'], loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.05))\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "_xf_wniEHwGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_angle = 45\n",
        "# rows, cols\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "ax1.plot(gas_actual, label='Actual Gas Price', color='blue')\n",
        "ax1.plot(gas_test_forecasted, label='Predicted Gas Price', color='orange')\n",
        "\n",
        "ax1.set_title('Actual vs Predicted Gas Price')\n",
        "# ax1.set_xlabel('Time')\n",
        "# ax1.set_ylabel('Gas')\n",
        "ax1.tick_params(axis='x', rotation=rotation_angle)\n",
        "ax1.set_ylim(0)\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(load_actual, label='Actual Load', color='blue')\n",
        "ax2.plot(load_test_forecasted, label='Predicted Load', color='orange')\n",
        "\n",
        "ax2.set_title('Actual vs Predicted Load')\n",
        "# ax2.set_xlabel('Time')\n",
        "# ax2.set_ylabel('Load')\n",
        "ax2.tick_params(axis='x', rotation=rotation_angle)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WVbmqm2sPBki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(actual)\n",
        "plt.plot(predicted)"
      ],
      "metadata": {
        "id": "KkYShM_LKVDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg4WnhmIoHVX"
      },
      "source": [
        "## Permutation feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGNqK5npFSup"
      },
      "outputs": [],
      "source": [
        "# valid_data = valid_data.drop(['Coal', 'Crude Oil', 'Uranium', 'Generation Scheduled', 'Wheel Throughs Bid',\n",
        "#        'Gross Imports NYISO', 'Gross Exports HQ'], axis=1)\n",
        "# train_data = train_data.drop(['Coal', 'Crude Oil', 'Uranium', 'Generation Scheduled', 'Wheel Throughs Bid',\n",
        "#        'Gross Imports NYISO', 'Gross Exports HQ'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1fYLlNqksP"
      },
      "outputs": [],
      "source": [
        "valid_data_y = valid_data[lbmp]\n",
        "valid_data = valid_data.drop(lbmp, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OSBd91qoKU7"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "filename = 'best_for_choosen_gen_4H_4folds|gradient_boosting_model.pkl'\n",
        "model_name = filename.split('|')[1].split('.')[0]\n",
        "# model = models_dict[model_name](features)\n",
        "# model.read_params(filename.split('.')[0])\n",
        "model = MyFbProphet(features)\n",
        "\n",
        "model.fit(train_data.drop([lbmp], axis=1), train_data[lbmp])\n",
        "predicted = model.predict(valid_data)\n",
        "curr_mape = mape(valid_data_y.values, predicted)\n",
        "# print(curr_mape)\n",
        "\n",
        "results.append({'feature': 'BASELINE','mape': curr_mape}) \n",
        "\n",
        "# for k in tqdm(range(len(COLS))):\n",
        "for k in range(len(features)):\n",
        "    \n",
        "    # SHUFFLE FEATURE K\n",
        "    save_col = valid_data.iloc[:, k].copy()\n",
        "    np.random.shuffle(valid_data.iloc[:, k])\n",
        "                        \n",
        "    # COMPUTE OOF MAE WITH FEATURE K SHUFFLED\n",
        "    predicted = model.predict(valid_data)#.squeeze() \n",
        "    res_mape = mape(valid_data_y.values, predicted)\n",
        "    results.append({'feature':features[k],'mape':res_mape})\n",
        "    valid_data.iloc[:, k] = save_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9GrHKI6s0GV"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)\n",
        "df = df.sort_values('mape')\n",
        "plt.figure(figsize=(4,7))\n",
        "plt.barh(np.arange(len(features)+1),df.mape)\n",
        "plt.yticks(np.arange(len(features)+1),df.feature.values)\n",
        "# plt.title('Feature Importance',size=16)\n",
        "plt.ylim((-1,len(features)+1))\n",
        "plt.plot([curr_mape,curr_mape],[-1,len(features)+1], '--', color='orange',\n",
        "                     label=f'Baseline\\nMAPE={curr_mape:.3f}')\n",
        "# plt.xlabel(f'Fold {fold+1} OOF MAE with feature permuted',size=14)\n",
        "plt.ylabel('Feature',size=14)\n",
        "plt.xlabel('MAPE',size=11)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYCS3c0onmhS"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)\n",
        "df = df.sort_values('mape')\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(np.arange(len(features)+1),df.mape)\n",
        "plt.xticks(np.arange(len(features)+1),df.feature.values, rotation=75)\n",
        "\n",
        "# plt.title('Feature Importance',size=16)\n",
        "plt.ylim((-1,40))\n",
        "plt.plot([-1,len(features)+1],\n",
        "         [curr_mape, curr_mape],\n",
        "         '--', color='orange',\n",
        "         label=f'Baseline\\nMAPE={curr_mape:.3f}')\n",
        "\n",
        "# plt.xlabel('Feature',size=14)\n",
        "plt.ylabel('MAPE',size=11)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNMKIYgFlcUg"
      },
      "source": [
        "## Lags analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKHbk-Pgl6Ff"
      },
      "outputs": [],
      "source": [
        "def create_lags_dataset(x, y, n_lags):\n",
        "    temp_df = pd.DataFrame({\"lag0\": x})\n",
        "    for cur_lag in range(1, n_lags + 1):\n",
        "        temp_df[\"lag{}\".format(cur_lag)] = x.shift(cur_lag)\n",
        "    return temp_df.iloc[n_lags:], y.iloc[n_lags:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apBYW7CRlgl4"
      },
      "outputs": [],
      "source": [
        "def calc_pacf(x, y):\n",
        "    pacf = [x[x.columns[0]].corr(y)]\n",
        "    for cur_lag in range(1, len(x.columns)):\n",
        "        model = LinearRegression()\n",
        "        cur_x = x[x.columns[:cur_lag]]\n",
        "        model.fit(cur_x, y)\n",
        "        residuals = y - model.predict(cur_x)\n",
        "        pacf.append(x[x.columns[cur_lag]].corr(residuals))\n",
        "    \n",
        "    return np.array(pacf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VzuDSChlvXW"
      },
      "outputs": [],
      "source": [
        "def get_feature_importance_df(independent_columns, use_zero_lag_list, target_column, n_lags, feature_importance_fn):\n",
        "    lags = list(range(n_lags + 1))\n",
        "    feature_importances_dict = {\"lag\": lags}\n",
        "    \n",
        "    for column, use_zero_lag in zip(independent_columns, use_zero_lag_list):\n",
        "        x, y = create_lags_dataset(column, target_column, n_lags)\n",
        "        if not use_zero_lag:\n",
        "            x.drop(\"lag0\", axis=1, inplace=True)\n",
        "        \n",
        "        feature_importances = feature_importance_fn(x, y)\n",
        "        if not use_zero_lag:\n",
        "            feature_importances = np.concatenate((np.array([np.NaN]), feature_importances))\n",
        "        \n",
        "        feature_importances_dict[column.name] = feature_importances\n",
        "    \n",
        "    return pd.DataFrame(feature_importances_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVcIuWWdmXQf"
      },
      "outputs": [],
      "source": [
        "columns = features\n",
        "use_zero_lag_list = [True] * len(columns)\n",
        "independent_columns = [data_df[column] for column in columns]\n",
        "target_column = data_df[lbmp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utK7664hl9zL"
      },
      "outputs": [],
      "source": [
        "n_lags = 100\n",
        "pacf_importances = get_feature_importance_df(independent_columns, use_zero_lag_list, target_column, n_lags, calc_pacf)\n",
        "pacf_importances.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BXzM3G4oBKv"
      },
      "outputs": [],
      "source": [
        "pacf_importances[columns].plot(figsize=(15, 8), legend=True, ylabel='PACF', xlabel='Lags')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVWXl_NsetO9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vO7BbI0v8OTL",
        "aq5L8vUdy-qp",
        "868_HJfZy-q9",
        "ZLvzeqpYSLsI",
        "Dg4WnhmIoHVX"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}